{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e98bd847-5146-4e75-b104-ee6c63c24162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import onnxruntime\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc314c27-0cb9-4b4b-8e90-ec3f9144c88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72bfdb88-889b-4b40-8130-96452d29a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    # Resize to currently hardcoded 1344x1344\n",
    "    image = image.resize((1344, 1344))\n",
    "    \n",
    "    # Convert to BGR\n",
    "    image = np.array(image)[:, :, [2, 1, 0]].astype('float32')\n",
    "\n",
    "    # HWC -> CHW\n",
    "    image = np.transpose(image, [2, 0, 1])\n",
    "\n",
    "    # Pad to be divisible of 32 - Might need this later on if I don't hardcode image size to 1344x1344\n",
    "    #import math\n",
    "    #padded_h = int(math.ceil(image.shape[1] / 32) * 32)\n",
    "    #padded_w = int(math.ceil(image.shape[2] / 32) * 32)\n",
    "\n",
    "    #padded_image = np.zeros((3, padded_h, padded_w), dtype=np.float32)\n",
    "    #padded_image[:, :image.shape[1], :image.shape[2]] = image\n",
    "    #image = padded_image\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "360b99a8-b65b-40f6-8405-8228f6fd29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/Users/henrylydecker/Documents/projects/aerial-segmentation/weights/urban_small_poc.onnx'\n",
    "onnx_model = onnx.load(model_name)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6540b5e-32e8-4203-9b59-dc4f98ec1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "image = Image.open(\"/Users/henrylydecker/Desktop/test.png\")\n",
    "\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(model_name)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "img = Image.open(\"/Users/henrylydecker/Desktop/test.png\")\n",
    "\n",
    "resize = transforms.Resize([224, 224])\n",
    "img = resize(img)\n",
    "\n",
    "img_ycbcr = img.convert('YCbCr')\n",
    "img_y, img_cb, img_cr = img_ycbcr.split()\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "img_y = to_tensor(img_y)\n",
    "img_y.unsqueeze_(0)\n",
    "\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "img_out_y = ort_outs[0]\n",
    "\n",
    "img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode='L')\n",
    "\n",
    "# get the output image follow post-processing step from PyTorch implementation\n",
    "final_img = Image.merge(\n",
    "    \"YCbCr\", [\n",
    "        img_out_y,\n",
    "        img_cb.resize(img_out_y.size, Image.BICUBIC),\n",
    "        img_cr.resize(img_out_y.size, Image.BICUBIC),\n",
    "    ]).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "304fd4e7-cfe5-422c-bfda-321e8c09085d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m img_data \u001b[38;5;241m=\u001b[39m preprocess(image)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Predict using ONNX Runtime - throw away unneeded outputs (Still trying to work out what the extra outputs actually are)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m boxes, labels, _, scores, _ \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28;01mNone\u001b[39;00m, {\n\u001b[1;32m     13\u001b[0m     session\u001b[38;5;241m.\u001b[39mget_inputs()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname: img_data\n\u001b[1;32m     14\u001b[0m })\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "session = onnxruntime.InferenceSession(\"/Users/henrylydecker/Documents/projects/aerial-segmentation/weights/urban_small_poc.onnx\")\n",
    "\n",
    "image = Image.open(\"/Users/henrylydecker/Desktop/test.png\")\n",
    "\n",
    "# Preprocess the image and convert to a numpy array of float:\n",
    "# 1. reinterpolate it to the hardcoded size\n",
    "# 2. reorder the colour axis as requires by the model (RGB -> BGR)\n",
    "# 3. reorder the pixel/color array (YXC -> CXY)\n",
    "img_data = preprocess(image)\n",
    "\n",
    "# Predict using ONNX Runtime - throw away unneeded outputs (Still trying to work out what the extra outputs actually are)\n",
    "boxes, labels, _, scores, _ = session.run(None, {\n",
    "    session.get_inputs()[0].name: img_data\n",
    "})\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
